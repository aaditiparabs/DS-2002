{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30cf167a",
   "metadata": {},
   "source": [
    "## Using Python to Perform Extract-Transform-Load (ETL Processing)\n",
    "Modern Data Warehousing and Analytics solutions frequently use languages like Python or Scala to extract data from numerous sources, including relational database management systems, NoSQL database systems, real-time streaming endpoints and Data Lakes.  These languages can then be used to perform many types of transformation before then loading the data into a variety of destinations including file systems and data warehouses. This data can then be consumed by data scientists or business analysts.\n",
    "\n",
    "In this lab you will recreate the **Northwind_DW** dimensional database from Lab 2; however, you'll take an entirely different approach. Instead of extracting, transforming and loading the date entirely on the database system only using SQL data definition language (DDL) and data manipulation language (DML) statements, here you will learn to interact with the RDBMS from a remote client running Python. You will learn to fetch data into Pandas DataFrames, perform all the necessary transformations in-memory on the client, and then push the newly transformed DataFrame back to the RDBMS using a Pandas function that will create the table and fill it with data with a single operation.\n",
    "\n",
    "### Prerequisites:\n",
    "This notebook uses the SqlAlchemy database connectivity library to connect to MySQL databases; therefore, you must have first installed that libary into your python environment by executing the following command in a Terminal window.\n",
    "\n",
    "- `python -m pip install sqlalchemy`\n",
    "\n",
    "#### Import the Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9f7fe77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T14:23:44.429262500Z",
     "start_time": "2026-02-11T14:23:43.029697600Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31522597",
   "metadata": {},
   "source": [
    "#### Declare & Assign Connection Variables for the MySQL Server & Databases with which You'll be Working "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c1a3519",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T14:23:50.700010500Z",
     "start_time": "2026-02-11T14:23:50.602930700Z"
    }
   },
   "outputs": [],
   "source": [
    "host_name = \"localhost\"\n",
    "port = \"3306\"\n",
    "user_id = \"root\"\n",
    "pwd = \"Passw0rd123\"\n",
    "\n",
    "src_dbname = \"northwind\"\n",
    "dst_dbname = \"northwind_dw2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca42abc",
   "metadata": {},
   "source": [
    "#### Define Functions for Getting Data From and Setting Data Into Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c9d366a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T14:23:54.121467500Z",
     "start_time": "2026-02-11T14:23:54.012206800Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataframe(user_id, pwd, host_name, db_name, sql_query):\n",
    "    conn_str = f\"mysql+pymysql://{user_id}:{pwd}@{host_name}/{db_name}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    connection = sqlEngine.connect()\n",
    "    dframe = pd.read_sql(sql_query, connection);\n",
    "    connection.close()\n",
    "    \n",
    "    return dframe\n",
    "\n",
    "\n",
    "def set_dataframe(user_id, pwd, host_name, db_name, df, table_name, pk_column, db_operation):\n",
    "    conn_str = f\"mysql+pymysql://{user_id}:{pwd}@{host_name}/{db_name}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    db_connection = sqlEngine.connect()\n",
    "    \n",
    "    '''Invoke the Pandas DataFrame .to_sql( ) function to either create, or append to, a table'''\n",
    "    if db_operation in ['insert', 'update']:\n",
    "        if db_operation.lower() == \"insert\":\n",
    "            df.to_sql(table_name, con=db_connection, index=False, if_exists='replace')\n",
    "            db_connection.execute(text(f\"ALTER TABLE {table_name} ADD {pk_column} INT AUTO_INCREMENT PRIMARY KEY FIRST;\"))\n",
    "                    \n",
    "        elif db_operation.lower() == \"update\":\n",
    "            df.to_sql(table_name, con=db_connection, index=False, if_exists='append')\n",
    "\n",
    "    else:\n",
    "        print(\"The value supplied to the 'db_operation' parameter must be either 'insert' or 'update'.\")\n",
    "    \n",
    "    db_connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ca9773",
   "metadata": {},
   "source": [
    "#### Create the New Data Warehouse database, and to Use it, Switch the Connection Context.\n",
    "Clearly, you won't get very far without having a database to work with. Here we demonstrate how we can *drop* a database if it already exists, and then *create* the new **northwind_dw2** database and *use* it as the target of all subsequent operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70548734",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T14:24:04.369443700Z",
     "start_time": "2026-02-11T14:24:00.562854900Z"
    }
   },
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "(pymysql.err.OperationalError) (1045, \"Access denied for user 'root'@'localhost' (using password: YES)\")\n(Background on this error at: https://sqlalche.me/e/20/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mOperationalError\u001B[39m                          Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:143\u001B[39m, in \u001B[36mConnection.__init__\u001B[39m\u001B[34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001B[39m\n\u001B[32m    142\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m143\u001B[39m     \u001B[38;5;28mself\u001B[39m._dbapi_connection = \u001B[43mengine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mraw_connection\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    144\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m dialect.loaded_dbapi.Error \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3309\u001B[39m, in \u001B[36mEngine.raw_connection\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   3288\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001B[39;00m\n\u001B[32m   3289\u001B[39m \n\u001B[32m   3290\u001B[39m \u001B[33;03mThe returned object is a proxied version of the DBAPI\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   3307\u001B[39m \n\u001B[32m   3308\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3309\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpool\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:447\u001B[39m, in \u001B[36mPool.connect\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    440\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Return a DBAPI connection from the pool.\u001B[39;00m\n\u001B[32m    441\u001B[39m \n\u001B[32m    442\u001B[39m \u001B[33;03mThe connection is instrumented such that when its\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    445\u001B[39m \n\u001B[32m    446\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m447\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_ConnectionFairy\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_checkout\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:1264\u001B[39m, in \u001B[36m_ConnectionFairy._checkout\u001B[39m\u001B[34m(cls, pool, threadconns, fairy)\u001B[39m\n\u001B[32m   1263\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m fairy:\n\u001B[32m-> \u001B[39m\u001B[32m1264\u001B[39m     fairy = \u001B[43m_ConnectionRecord\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcheckout\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpool\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1266\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m threadconns \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:711\u001B[39m, in \u001B[36m_ConnectionRecord.checkout\u001B[39m\u001B[34m(cls, pool)\u001B[39m\n\u001B[32m    710\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m711\u001B[39m     rec = \u001B[43mpool\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_do_get\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    713\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:177\u001B[39m, in \u001B[36mQueuePool._do_get\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    176\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m177\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mutil\u001B[49m\u001B[43m.\u001B[49m\u001B[43msafe_reraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    178\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dec_overflow\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:224\u001B[39m, in \u001B[36msafe_reraise.__exit__\u001B[39m\u001B[34m(self, type_, value, traceback)\u001B[39m\n\u001B[32m    223\u001B[39m     \u001B[38;5;28mself\u001B[39m._exc_info = \u001B[38;5;28;01mNone\u001B[39;00m  \u001B[38;5;66;03m# remove potential circular references\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m224\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc_value.with_traceback(exc_tb)\n\u001B[32m    225\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:175\u001B[39m, in \u001B[36mQueuePool._do_get\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    174\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m175\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_create_connection\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    176\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:388\u001B[39m, in \u001B[36mPool._create_connection\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    386\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m388\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_ConnectionRecord\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:673\u001B[39m, in \u001B[36m_ConnectionRecord.__init__\u001B[39m\u001B[34m(self, pool, connect)\u001B[39m\n\u001B[32m    672\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m connect:\n\u001B[32m--> \u001B[39m\u001B[32m673\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m__connect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    674\u001B[39m \u001B[38;5;28mself\u001B[39m.finalize_callback = deque()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:899\u001B[39m, in \u001B[36m_ConnectionRecord.__connect\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    898\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m--> \u001B[39m\u001B[32m899\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mutil\u001B[49m\u001B[43m.\u001B[49m\u001B[43msafe_reraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    900\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpool\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlogger\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdebug\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mError on connect(): \u001B[39;49m\u001B[38;5;132;43;01m%s\u001B[39;49;00m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43me\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:224\u001B[39m, in \u001B[36msafe_reraise.__exit__\u001B[39m\u001B[34m(self, type_, value, traceback)\u001B[39m\n\u001B[32m    223\u001B[39m     \u001B[38;5;28mself\u001B[39m._exc_info = \u001B[38;5;28;01mNone\u001B[39;00m  \u001B[38;5;66;03m# remove potential circular references\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m224\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc_value.with_traceback(exc_tb)\n\u001B[32m    225\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:895\u001B[39m, in \u001B[36m_ConnectionRecord.__connect\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    894\u001B[39m \u001B[38;5;28mself\u001B[39m.starttime = time.time()\n\u001B[32m--> \u001B[39m\u001B[32m895\u001B[39m \u001B[38;5;28mself\u001B[39m.dbapi_connection = connection = \u001B[43mpool\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_invoke_creator\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    896\u001B[39m pool.logger.debug(\u001B[33m\"\u001B[39m\u001B[33mCreated new connection \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[33m\"\u001B[39m, connection)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\create.py:661\u001B[39m, in \u001B[36mcreate_engine.<locals>.connect\u001B[39m\u001B[34m(connection_record)\u001B[39m\n\u001B[32m    659\u001B[39m             \u001B[38;5;28;01mreturn\u001B[39;00m connection\n\u001B[32m--> \u001B[39m\u001B[32m661\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdialect\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43mcargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mcparams\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:630\u001B[39m, in \u001B[36mDefaultDialect.connect\u001B[39m\u001B[34m(self, *cargs, **cparams)\u001B[39m\n\u001B[32m    628\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mconnect\u001B[39m(\u001B[38;5;28mself\u001B[39m, *cargs: Any, **cparams: Any) -> DBAPIConnection:\n\u001B[32m    629\u001B[39m     \u001B[38;5;66;03m# inherits the docstring from interfaces.Dialect.connect\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m630\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mloaded_dbapi\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43mcargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mcparams\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\pymysql\\connections.py:365\u001B[39m, in \u001B[36mConnection.__init__\u001B[39m\u001B[34m(self, user, password, host, database, unix_socket, port, charset, collation, sql_mode, read_default_file, conv, use_unicode, client_flag, cursorclass, init_command, connect_timeout, read_default_group, autocommit, local_infile, max_allowed_packet, defer_connect, auth_plugin_map, read_timeout, write_timeout, bind_address, binary_prefix, program_name, server_public_key, ssl, ssl_ca, ssl_cert, ssl_disabled, ssl_key, ssl_key_password, ssl_verify_cert, ssl_verify_identity, compress, named_pipe, passwd, db)\u001B[39m\n\u001B[32m    364\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m365\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\pymysql\\connections.py:681\u001B[39m, in \u001B[36mConnection.connect\u001B[39m\u001B[34m(self, sock)\u001B[39m\n\u001B[32m    680\u001B[39m \u001B[38;5;28mself\u001B[39m._get_server_information()\n\u001B[32m--> \u001B[39m\u001B[32m681\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_request_authentication\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    683\u001B[39m \u001B[38;5;66;03m# Send \"SET NAMES\" query on init for:\u001B[39;00m\n\u001B[32m    684\u001B[39m \u001B[38;5;66;03m# - Ensure charaset (and collation) is set to the server.\u001B[39;00m\n\u001B[32m    685\u001B[39m \u001B[38;5;66;03m#   - collation_id in handshake packet may be ignored.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    694\u001B[39m \u001B[38;5;66;03m# - https://github.com/wagtail/wagtail/issues/9477\u001B[39;00m\n\u001B[32m    695\u001B[39m \u001B[38;5;66;03m# - https://zenn.dev/methane/articles/2023-mysql-collation (Japanese)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\pymysql\\connections.py:980\u001B[39m, in \u001B[36mConnection._request_authentication\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    979\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._auth_plugin_name == \u001B[33m\"\u001B[39m\u001B[33mcaching_sha2_password\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m980\u001B[39m     auth_packet = \u001B[43m_auth\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcaching_sha2_password_auth\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mauth_packet\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    981\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._auth_plugin_name == \u001B[33m\"\u001B[39m\u001B[33msha256_password\u001B[39m\u001B[33m\"\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\pymysql\\_auth.py:272\u001B[39m, in \u001B[36mcaching_sha2_password_auth\u001B[39m\u001B[34m(conn, pkt)\u001B[39m\n\u001B[32m    271\u001B[39m data = sha2_rsa_encrypt(conn.password, conn.salt, conn.server_public_key)\n\u001B[32m--> \u001B[39m\u001B[32m272\u001B[39m pkt = \u001B[43m_roundtrip\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\pymysql\\_auth.py:121\u001B[39m, in \u001B[36m_roundtrip\u001B[39m\u001B[34m(conn, send_data)\u001B[39m\n\u001B[32m    120\u001B[39m conn.write_packet(send_data)\n\u001B[32m--> \u001B[39m\u001B[32m121\u001B[39m pkt = \u001B[43mconn\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_read_packet\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    122\u001B[39m pkt.check_error()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\pymysql\\connections.py:782\u001B[39m, in \u001B[36mConnection._read_packet\u001B[39m\u001B[34m(self, packet_type)\u001B[39m\n\u001B[32m    781\u001B[39m         \u001B[38;5;28mself\u001B[39m._result.unbuffered_active = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m782\u001B[39m     \u001B[43mpacket\u001B[49m\u001B[43m.\u001B[49m\u001B[43mraise_for_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    783\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m packet\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\pymysql\\protocol.py:219\u001B[39m, in \u001B[36mMysqlPacket.raise_for_error\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    218\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33merrno =\u001B[39m\u001B[33m\"\u001B[39m, errno)\n\u001B[32m--> \u001B[39m\u001B[32m219\u001B[39m \u001B[43merr\u001B[49m\u001B[43m.\u001B[49m\u001B[43mraise_mysql_exception\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_data\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\pymysql\\err.py:150\u001B[39m, in \u001B[36mraise_mysql_exception\u001B[39m\u001B[34m(data)\u001B[39m\n\u001B[32m    149\u001B[39m     errorclass = InternalError \u001B[38;5;28;01mif\u001B[39;00m errno < \u001B[32m1000\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m OperationalError\n\u001B[32m--> \u001B[39m\u001B[32m150\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m errorclass(errno, errval)\n",
      "\u001B[31mOperationalError\u001B[39m: (1045, \"Access denied for user 'root'@'localhost' (using password: YES)\")",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mOperationalError\u001B[39m                          Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m conn_str = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mmysql+pymysql://\u001B[39m\u001B[38;5;132;01m{\u001B[39;00muser_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m:\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpwd\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m@\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhost_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m      2\u001B[39m sqlEngine = create_engine(conn_str, pool_recycle=\u001B[32m3600\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m connection = \u001B[43msqlEngine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      5\u001B[39m connection.execute(text(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mDROP DATABASE IF EXISTS `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdst_dbname\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m`;\u001B[39m\u001B[33m\"\u001B[39m))\n\u001B[32m      6\u001B[39m connection.execute(text(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCREATE DATABASE `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdst_dbname\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m`;\u001B[39m\u001B[33m\"\u001B[39m))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3285\u001B[39m, in \u001B[36mEngine.connect\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   3262\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mconnect\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> Connection:\n\u001B[32m   3263\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Return a new :class:`_engine.Connection` object.\u001B[39;00m\n\u001B[32m   3264\u001B[39m \n\u001B[32m   3265\u001B[39m \u001B[33;03m    The :class:`_engine.Connection` acts as a Python context manager, so\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   3282\u001B[39m \n\u001B[32m   3283\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3285\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_connection_cls\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:145\u001B[39m, in \u001B[36mConnection.__init__\u001B[39m\u001B[34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001B[39m\n\u001B[32m    143\u001B[39m         \u001B[38;5;28mself\u001B[39m._dbapi_connection = engine.raw_connection()\n\u001B[32m    144\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m dialect.loaded_dbapi.Error \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[32m--> \u001B[39m\u001B[32m145\u001B[39m         \u001B[43mConnection\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_handle_dbapi_exception_noconnection\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    146\u001B[39m \u001B[43m            \u001B[49m\u001B[43merr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdialect\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mengine\u001B[49m\n\u001B[32m    147\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    148\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[32m    149\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:2448\u001B[39m, in \u001B[36mConnection._handle_dbapi_exception_noconnection\u001B[39m\u001B[34m(cls, e, dialect, engine, is_disconnect, invalidate_pool_on_disconnect, is_pre_ping)\u001B[39m\n\u001B[32m   2446\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m should_wrap:\n\u001B[32m   2447\u001B[39m     \u001B[38;5;28;01massert\u001B[39;00m sqlalchemy_exception \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2448\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m sqlalchemy_exception.with_traceback(exc_info[\u001B[32m2\u001B[39m]) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01me\u001B[39;00m\n\u001B[32m   2449\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   2450\u001B[39m     \u001B[38;5;28;01massert\u001B[39;00m exc_info[\u001B[32m1\u001B[39m] \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:143\u001B[39m, in \u001B[36mConnection.__init__\u001B[39m\u001B[34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001B[39m\n\u001B[32m    141\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m connection \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    142\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m143\u001B[39m         \u001B[38;5;28mself\u001B[39m._dbapi_connection = \u001B[43mengine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mraw_connection\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    144\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m dialect.loaded_dbapi.Error \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[32m    145\u001B[39m         Connection._handle_dbapi_exception_noconnection(\n\u001B[32m    146\u001B[39m             err, dialect, engine\n\u001B[32m    147\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:3309\u001B[39m, in \u001B[36mEngine.raw_connection\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   3287\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mraw_connection\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> PoolProxiedConnection:\n\u001B[32m   3288\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001B[39;00m\n\u001B[32m   3289\u001B[39m \n\u001B[32m   3290\u001B[39m \u001B[33;03m    The returned object is a proxied version of the DBAPI\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   3307\u001B[39m \n\u001B[32m   3308\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m3309\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpool\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:447\u001B[39m, in \u001B[36mPool.connect\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    439\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mconnect\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> PoolProxiedConnection:\n\u001B[32m    440\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Return a DBAPI connection from the pool.\u001B[39;00m\n\u001B[32m    441\u001B[39m \n\u001B[32m    442\u001B[39m \u001B[33;03m    The connection is instrumented such that when its\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    445\u001B[39m \n\u001B[32m    446\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m447\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_ConnectionFairy\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_checkout\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:1264\u001B[39m, in \u001B[36m_ConnectionFairy._checkout\u001B[39m\u001B[34m(cls, pool, threadconns, fairy)\u001B[39m\n\u001B[32m   1256\u001B[39m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[32m   1257\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_checkout\u001B[39m(\n\u001B[32m   1258\u001B[39m     \u001B[38;5;28mcls\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1261\u001B[39m     fairy: Optional[_ConnectionFairy] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1262\u001B[39m ) -> _ConnectionFairy:\n\u001B[32m   1263\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m fairy:\n\u001B[32m-> \u001B[39m\u001B[32m1264\u001B[39m         fairy = \u001B[43m_ConnectionRecord\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcheckout\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpool\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1266\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m threadconns \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1267\u001B[39m             threadconns.current = weakref.ref(fairy)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:711\u001B[39m, in \u001B[36m_ConnectionRecord.checkout\u001B[39m\u001B[34m(cls, pool)\u001B[39m\n\u001B[32m    709\u001B[39m     rec = cast(_ConnectionRecord, pool._do_get())\n\u001B[32m    710\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m711\u001B[39m     rec = \u001B[43mpool\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_do_get\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    713\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    714\u001B[39m     dbapi_connection = rec.get_connection()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:177\u001B[39m, in \u001B[36mQueuePool._do_get\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    175\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._create_connection()\n\u001B[32m    176\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m177\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mutil\u001B[49m\u001B[43m.\u001B[49m\u001B[43msafe_reraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    178\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dec_overflow\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    179\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:224\u001B[39m, in \u001B[36msafe_reraise.__exit__\u001B[39m\u001B[34m(self, type_, value, traceback)\u001B[39m\n\u001B[32m    222\u001B[39m     \u001B[38;5;28;01massert\u001B[39;00m exc_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    223\u001B[39m     \u001B[38;5;28mself\u001B[39m._exc_info = \u001B[38;5;28;01mNone\u001B[39;00m  \u001B[38;5;66;03m# remove potential circular references\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m224\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc_value.with_traceback(exc_tb)\n\u001B[32m    225\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    226\u001B[39m     \u001B[38;5;28mself\u001B[39m._exc_info = \u001B[38;5;28;01mNone\u001B[39;00m  \u001B[38;5;66;03m# remove potential circular references\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\impl.py:175\u001B[39m, in \u001B[36mQueuePool._do_get\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    173\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._inc_overflow():\n\u001B[32m    174\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m175\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_create_connection\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    176\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[32m    177\u001B[39m         \u001B[38;5;28;01mwith\u001B[39;00m util.safe_reraise():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:388\u001B[39m, in \u001B[36mPool._create_connection\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    385\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_create_connection\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> ConnectionPoolEntry:\n\u001B[32m    386\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m388\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_ConnectionRecord\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:673\u001B[39m, in \u001B[36m_ConnectionRecord.__init__\u001B[39m\u001B[34m(self, pool, connect)\u001B[39m\n\u001B[32m    671\u001B[39m \u001B[38;5;28mself\u001B[39m.__pool = pool\n\u001B[32m    672\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m connect:\n\u001B[32m--> \u001B[39m\u001B[32m673\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m__connect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    674\u001B[39m \u001B[38;5;28mself\u001B[39m.finalize_callback = deque()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:899\u001B[39m, in \u001B[36m_ConnectionRecord.__connect\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    897\u001B[39m     \u001B[38;5;28mself\u001B[39m.fresh = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    898\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m--> \u001B[39m\u001B[32m899\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mutil\u001B[49m\u001B[43m.\u001B[49m\u001B[43msafe_reraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m    900\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpool\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlogger\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdebug\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mError on connect(): \u001B[39;49m\u001B[38;5;132;43;01m%s\u001B[39;49;00m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43me\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    901\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    902\u001B[39m     \u001B[38;5;66;03m# in SQLAlchemy 1.4 the first_connect event is not used by\u001B[39;00m\n\u001B[32m    903\u001B[39m     \u001B[38;5;66;03m# the engine, so this will usually not be set\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\util\\langhelpers.py:224\u001B[39m, in \u001B[36msafe_reraise.__exit__\u001B[39m\u001B[34m(self, type_, value, traceback)\u001B[39m\n\u001B[32m    222\u001B[39m     \u001B[38;5;28;01massert\u001B[39;00m exc_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    223\u001B[39m     \u001B[38;5;28mself\u001B[39m._exc_info = \u001B[38;5;28;01mNone\u001B[39;00m  \u001B[38;5;66;03m# remove potential circular references\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m224\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m exc_value.with_traceback(exc_tb)\n\u001B[32m    225\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    226\u001B[39m     \u001B[38;5;28mself\u001B[39m._exc_info = \u001B[38;5;28;01mNone\u001B[39;00m  \u001B[38;5;66;03m# remove potential circular references\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\pool\\base.py:895\u001B[39m, in \u001B[36m_ConnectionRecord.__connect\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    893\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    894\u001B[39m     \u001B[38;5;28mself\u001B[39m.starttime = time.time()\n\u001B[32m--> \u001B[39m\u001B[32m895\u001B[39m     \u001B[38;5;28mself\u001B[39m.dbapi_connection = connection = \u001B[43mpool\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_invoke_creator\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    896\u001B[39m     pool.logger.debug(\u001B[33m\"\u001B[39m\u001B[33mCreated new connection \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[33m\"\u001B[39m, connection)\n\u001B[32m    897\u001B[39m     \u001B[38;5;28mself\u001B[39m.fresh = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\create.py:661\u001B[39m, in \u001B[36mcreate_engine.<locals>.connect\u001B[39m\u001B[34m(connection_record)\u001B[39m\n\u001B[32m    658\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m connection \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    659\u001B[39m             \u001B[38;5;28;01mreturn\u001B[39;00m connection\n\u001B[32m--> \u001B[39m\u001B[32m661\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdialect\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43mcargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mcparams\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:630\u001B[39m, in \u001B[36mDefaultDialect.connect\u001B[39m\u001B[34m(self, *cargs, **cparams)\u001B[39m\n\u001B[32m    628\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mconnect\u001B[39m(\u001B[38;5;28mself\u001B[39m, *cargs: Any, **cparams: Any) -> DBAPIConnection:\n\u001B[32m    629\u001B[39m     \u001B[38;5;66;03m# inherits the docstring from interfaces.Dialect.connect\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m630\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mloaded_dbapi\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43mcargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mcparams\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\pymysql\\connections.py:365\u001B[39m, in \u001B[36mConnection.__init__\u001B[39m\u001B[34m(self, user, password, host, database, unix_socket, port, charset, collation, sql_mode, read_default_file, conv, use_unicode, client_flag, cursorclass, init_command, connect_timeout, read_default_group, autocommit, local_infile, max_allowed_packet, defer_connect, auth_plugin_map, read_timeout, write_timeout, bind_address, binary_prefix, program_name, server_public_key, ssl, ssl_ca, ssl_cert, ssl_disabled, ssl_key, ssl_key_password, ssl_verify_cert, ssl_verify_identity, compress, named_pipe, passwd, db)\u001B[39m\n\u001B[32m    363\u001B[39m     \u001B[38;5;28mself\u001B[39m._sock = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    364\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m365\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\pymysql\\connections.py:681\u001B[39m, in \u001B[36mConnection.connect\u001B[39m\u001B[34m(self, sock)\u001B[39m\n\u001B[32m    678\u001B[39m \u001B[38;5;28mself\u001B[39m._next_seq_id = \u001B[32m0\u001B[39m\n\u001B[32m    680\u001B[39m \u001B[38;5;28mself\u001B[39m._get_server_information()\n\u001B[32m--> \u001B[39m\u001B[32m681\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_request_authentication\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    683\u001B[39m \u001B[38;5;66;03m# Send \"SET NAMES\" query on init for:\u001B[39;00m\n\u001B[32m    684\u001B[39m \u001B[38;5;66;03m# - Ensure charaset (and collation) is set to the server.\u001B[39;00m\n\u001B[32m    685\u001B[39m \u001B[38;5;66;03m#   - collation_id in handshake packet may be ignored.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    694\u001B[39m \u001B[38;5;66;03m# - https://github.com/wagtail/wagtail/issues/9477\u001B[39;00m\n\u001B[32m    695\u001B[39m \u001B[38;5;66;03m# - https://zenn.dev/methane/articles/2023-mysql-collation (Japanese)\u001B[39;00m\n\u001B[32m    696\u001B[39m \u001B[38;5;28mself\u001B[39m.set_character_set(\u001B[38;5;28mself\u001B[39m.charset, \u001B[38;5;28mself\u001B[39m.collation)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\pymysql\\connections.py:980\u001B[39m, in \u001B[36mConnection._request_authentication\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    978\u001B[39m \u001B[38;5;66;03m# https://dev.mysql.com/doc/internals/en/successful-authentication.html\u001B[39;00m\n\u001B[32m    979\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._auth_plugin_name == \u001B[33m\"\u001B[39m\u001B[33mcaching_sha2_password\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m980\u001B[39m     auth_packet = \u001B[43m_auth\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcaching_sha2_password_auth\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mauth_packet\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    981\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._auth_plugin_name == \u001B[33m\"\u001B[39m\u001B[33msha256_password\u001B[39m\u001B[33m\"\u001B[39m:\n\u001B[32m    982\u001B[39m     auth_packet = _auth.sha256_password_auth(\u001B[38;5;28mself\u001B[39m, auth_packet)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\pymysql\\_auth.py:272\u001B[39m, in \u001B[36mcaching_sha2_password_auth\u001B[39m\u001B[34m(conn, pkt)\u001B[39m\n\u001B[32m    269\u001B[39m         \u001B[38;5;28mprint\u001B[39m(conn.server_public_key.decode(\u001B[33m\"\u001B[39m\u001B[33mascii\u001B[39m\u001B[33m\"\u001B[39m))\n\u001B[32m    271\u001B[39m data = sha2_rsa_encrypt(conn.password, conn.salt, conn.server_public_key)\n\u001B[32m--> \u001B[39m\u001B[32m272\u001B[39m pkt = \u001B[43m_roundtrip\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\pymysql\\_auth.py:121\u001B[39m, in \u001B[36m_roundtrip\u001B[39m\u001B[34m(conn, send_data)\u001B[39m\n\u001B[32m    119\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_roundtrip\u001B[39m(conn, send_data):\n\u001B[32m    120\u001B[39m     conn.write_packet(send_data)\n\u001B[32m--> \u001B[39m\u001B[32m121\u001B[39m     pkt = \u001B[43mconn\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_read_packet\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    122\u001B[39m     pkt.check_error()\n\u001B[32m    123\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m pkt\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\pymysql\\connections.py:782\u001B[39m, in \u001B[36mConnection._read_packet\u001B[39m\u001B[34m(self, packet_type)\u001B[39m\n\u001B[32m    780\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._result.unbuffered_active \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m    781\u001B[39m         \u001B[38;5;28mself\u001B[39m._result.unbuffered_active = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m782\u001B[39m     \u001B[43mpacket\u001B[49m\u001B[43m.\u001B[49m\u001B[43mraise_for_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    783\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m packet\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\pymysql\\protocol.py:219\u001B[39m, in \u001B[36mMysqlPacket.raise_for_error\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    217\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m DEBUG:\n\u001B[32m    218\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33merrno =\u001B[39m\u001B[33m\"\u001B[39m, errno)\n\u001B[32m--> \u001B[39m\u001B[32m219\u001B[39m \u001B[43merr\u001B[49m\u001B[43m.\u001B[49m\u001B[43mraise_mysql_exception\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_data\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Downloads\\dssystems\\DS-2002\\venv\\Lib\\site-packages\\pymysql\\err.py:150\u001B[39m, in \u001B[36mraise_mysql_exception\u001B[39m\u001B[34m(data)\u001B[39m\n\u001B[32m    148\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m errorclass \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    149\u001B[39m     errorclass = InternalError \u001B[38;5;28;01mif\u001B[39;00m errno < \u001B[32m1000\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m OperationalError\n\u001B[32m--> \u001B[39m\u001B[32m150\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m errorclass(errno, errval)\n",
      "\u001B[31mOperationalError\u001B[39m: (pymysql.err.OperationalError) (1045, \"Access denied for user 'root'@'localhost' (using password: YES)\")\n(Background on this error at: https://sqlalche.me/e/20/e3q8)"
     ]
    }
   ],
   "source": [
    "conn_str = f\"mysql+pymysql://{user_id}:{pwd}@{host_name}\"\n",
    "sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "connection = sqlEngine.connect()\n",
    "\n",
    "connection.execute(text(f\"DROP DATABASE IF EXISTS `{dst_dbname}`;\"))\n",
    "connection.execute(text(f\"CREATE DATABASE `{dst_dbname}`;\"))\n",
    "connection.execute(text(f\"USE {dst_dbname};\"))\n",
    "\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02169776",
   "metadata": {},
   "source": [
    "### 1.0. Create & Populate the Dimension Tables\n",
    "In any extract-transform-load (ETL) process used to populate a multi-dimensional data warehouse database it is necessary to populate the Dimension tables before attempting to populate the Fact table(s). This is because rows in the Fact table(s) will reference surrogate primary key values from the Dimension tables. If the primary key values in the Dimension tables either do not exist, or do not reflect the current state of the dimension, then the attempt to load the Fact table(s) will fail.\n",
    "\n",
    "\n",
    "#### 1.1. Extract Data from the Source Database Tables\n",
    "Fetch data for each dimension table (e.g., customers, employees, products, shippers) from the **northwind** database using the **get_dataframe()** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efbe3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_customers = \"SELECT * FROM northwind.customers;\"\n",
    "df_customers = get_dataframe(user_id, pwd, host_name, src_dbname, sql_customers)\n",
    "df_customers.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a278a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Employees\n",
    "sql_employees = \"SELECT * FROM northwind.employees;\"\n",
    "df_employees = get_dataframe(user_id, pwd, host_name, src_dbname, sql_employees)\n",
    "df_employees.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8da1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a13566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shippers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79743230",
   "metadata": {},
   "source": [
    "#### 1.2. Create the Date Dimension Table\n",
    "At this point, we have to **execute the script from Lab 2c** that creates and populates a **Date Dimension** table.  Be certain to target this script to the new data warehouse database we just created **(northwind_dw2)**.  Later in this notebook we will integrate the **dim_date** table with the fact table by performing **lookup operations** to retrieve the surrogate primary keys from the **dim_date** table that correspond with each **date** typed column in the fact table (e.g., order_date)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1900ab3f",
   "metadata": {},
   "source": [
    "#### 1.3. Perform Any Necessary Transformations\n",
    "Pandas DataFrames enable extensive data modification capabilities. Here we will start by simply dropping features (columns) that we don't believe provide any real value to our analytics solution. Examples include columns having a high percentage of NULL values, columns having large amounts of free-text, and columns having binary large object (BLOB) data such as images or other documents. Then, we will rename the primary key column from the source (id) to serve as the business key for future lookup operations. Finally, we will *insert* a new primary key column that contains and ever-increasing numeric value.  It should be named after the entity (e.g., customer, product) followed by \"**_key**\" to conform with data warehouse design standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09c57ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a List that enumerates the names of each column you wish to remove (drop) from the Pandas DataFrame\n",
    "drop_cols = ['email_address','home_phone','mobile_phone','web_page','notes','attachments']\n",
    "df_customers.drop(drop_cols, axis=1, inplace=True)\n",
    "\n",
    "# 2. Rename the \"id\" column to reflect the entity as it will serve as the business key for lookup operations\n",
    "df_customers.rename(columns={\"id\":\"customer_id\"}, inplace=True)\n",
    "\n",
    "# 3. Display the first 2 rows of the dataframe to validate your work\n",
    "df_customers.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a618aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d90d322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4bf2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shippers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cecaeb",
   "metadata": {},
   "source": [
    "#### 1.4. Load the Transformed DataFrames into the New Data Warehouse by Creating New Tables\n",
    "Here we demonstrate how an iterable data structure can be created containing the values needed to correctly create and populate the new dimension tables. If you inspect this code listing carefully, you'll notice that it's a **list** containing a **set** *(or vector)* for each dimension table. Each **set** then contains the *table_name* we need to assign to the table, the *pandas DataFrame* we crafted to define & populate the table, and the name we need to assign to the *primary_key* column.  With this *list of sets* defined, we can then call our **set_dataframe( )** function from within a **for *loop*** to create each *dimension* table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b84457",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_operation = \"insert\"\n",
    "\n",
    "tables = [('dim_customers', df_customers, 'customer_key'),\n",
    "          ('dim_employees', df_employees, 'employee_key'),\n",
    "          ('dim_products', df_products, 'product_key'),\n",
    "          ('dim_shippers', df_shippers, 'shipper_key')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74821a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "for table_name, dataframe, primary_key in tables:\n",
    "    set_dataframe(user_id, pwd, host_name, dst_dbname, dataframe, table_name, primary_key, db_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476dc940",
   "metadata": {},
   "source": [
    "### 2.0. Create & Populate the Fact Table\n",
    "Here we will learn two approaches to creating the *fact_orders* fact table. The first approach demonstrates that a carefully crafted SQL SELECT statement can be used to perform this task... *but what fun would that be.* Seriously though, this approach is quick and effect if you already have the query, but what if you didn't have the opportunity to view and work with the data beforehand?  What's more, you may be required to combine data from multiple sources, some of which may not be relational database management systems. Then, a simple SQL query won't do!  You would need to load the data from the various sources (e.g., database tables, CSV or JSON files, NoSQL document collections, API stream data) and then combine them into a single dataframe that you could then use to create a new database table. For this reason we'll see how we can retrieve the data, but we won't bother to use it for creating a new table... we already know how to do that using the **set_dataframe( )** function anyway.\n",
    "\n",
    "### First, you *could* simply use the SQL SELECT statement you authored in Lab 2 \n",
    "Just as we could create a new table using the SQL \"CREATE TABLE AS SELECT...\" or CTAS construct, it is possible to create new tables simply by crafting a new SQL result set and using it to populate a new Pandas DataFrame.  **However, this wouldn't demonstrate the power inherent to Pandas DataFrames.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16ff332",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_fact_orders = \"\"\"\n",
    "    SELECT o.id AS order_id,\n",
    "        od.id AS order_detail_id,\n",
    "        o.customer_id,\n",
    "        o.employee_id,\n",
    "        od.product_id,\n",
    "        o.shipper_id,\n",
    "        o.order_date,\n",
    "        o.paid_date,\n",
    "        o.shipped_date,\n",
    "        o.payment_type,\n",
    "        od.quantity,\n",
    "        od.unit_price,\n",
    "        od.discount,\n",
    "        o.shipping_fee,\n",
    "        o.taxes,\n",
    "        o.tax_rate,\n",
    "        os.status_name AS order_status,\n",
    "        ods.status_name AS order_details_status\n",
    "    FROM northwind.orders AS o\n",
    "    INNER JOIN northwind.orders_status AS os\n",
    "    ON o.status_id = os.id\n",
    "    RIGHT OUTER JOIN northwind.order_details AS od\n",
    "    ON o.id = od.order_id\n",
    "    INNER JOIN northwind.order_details_status AS ods\n",
    "    ON od.status_id = ods.id;\n",
    "\"\"\"\n",
    "\n",
    "df_fact_orders = get_dataframe(user_id, pwd, host_name, src_dbname, sql_fact_orders)\n",
    "df_fact_orders.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d739c0ad",
   "metadata": {},
   "source": [
    "### Instead, implement the solution using Pandas DataFrames to craft the table\n",
    "This is where we get to the point of this lab... *transforming in-memory data.*   First, we'll query the source **northwind** database to fill a *dataframe* for each of the source tables we need to create our *fact_orders* fact table; orders, orders_status, order_details and order_details_status. Then, we'll learn how to *join* those *dataframes* using the <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.merge.html\">**merge( )**</a> method of the Pandas DataFrame.  We'll make any additional changes that we expect to see reflected in the *fact* table in our new MySQL database, including the addition of **foreign key references** to the dimension tables, and then we'll push the *dataframe* back to the MySQL server to create and populate the new *fact* table.\n",
    "\n",
    "#### 2.1. Get all the data from each of the four tables involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17596425",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_orders = \"SELECT * FROM northwind.orders;\"\n",
    "df_orders = get_dataframe(user_id, pwd, host_name, src_dbname, sql_orders)\n",
    "df_orders.rename(columns={\"id\":\"order_id\"}, inplace=True)\n",
    "df_orders.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b138ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. SELECT all columns from the northwind.orders_status table to create the \"df_order_status\" dataframe\n",
    "# 2. Rename the \"id\" column to \"status_id\"\n",
    "# 3. Display the first two rows of the DataFrame to validate your work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a24eed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. SELECT all columns from the northwind.order_details table to create the \"df_order_details\" dataframe\n",
    "# 2. Rename the \"id\" column to \"order_detail_id\"\n",
    "# 3. Display the first two rows of the DataFrame to validate your work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe7eb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. SELECT all columns from the northwind.order_details_status table to create the \"df_order_details_status\" dataframe\n",
    "# 2. Rename the \"id\" column to \"status_id\"\n",
    "# 3. Display the first two rows of the DataFrame to validate your work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bacaed6",
   "metadata": {},
   "source": [
    "#### 2.2. Get the order_status column.\n",
    "Here we use the dataframe's <a href=\"https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.merge.html\">**merge(** *'left dataframe', 'right dataframe', on='key column', how='left' | 'right' | 'inner'* **)**</a> method to **inner join** the *orders* and the *orders_status* dataframes **on** the *status_id* column.  We then use the dataframe's **rename( )** method to rename the *status_name* column to *order_status*, and use the dataframe's **drop( )** method to remove the *status_id* column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0f524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders = pd.merge(df_orders, df_orders_status, on='status_id', how='inner')\n",
    "df_orders.rename(columns={\"status_name\":\"order_status\"}, inplace=True)\n",
    "df_orders.drop(['status_id'], axis=1, inplace=True)\n",
    "df_orders.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8020d2fb",
   "metadata": {},
   "source": [
    "#### 2.3. Get the order_details_status column.\n",
    "Here we **repeat the sequence of operations we used in the previous step** to *inner join* the *order_details* and *order_details_status* dataframes for the sake of including the *order_details_status* column in place of the *status_id* column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56ee5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e810fc64",
   "metadata": {},
   "source": [
    "#### 2.4. Join the Orders and OrderDetails DataFrames\n",
    "In this step we can now easily join the *orders* and *order_details* dataframes. Since each **order** (the *left* dataframe) can have many **order details** (the *right* dataframe), we'll need to implement a **right** *outer join* **on** the *order_id* column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8503ce28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7117fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_orders.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe324b94",
   "metadata": {},
   "source": [
    "#### 2.5. Lookup the Primary Keys from the Dimension Tables\n",
    "Just as we did in **Lab 1**, we need to establish **foreign key relationships** between the newly-crafted **Fact table** and each of the **Dimension tables**.\n",
    "\n",
    "##### 2.5.1. Fetch the Primary Key and Business Key from the Date Dimension Table.\n",
    "First, fetch the Surrogate Primary Key (date_key) and the Business Key (full_date) from the Date Dimension table using the **get_dataframe()** function. Be certain to cast the **full_date** column to the **datetime64[ns]** data type using the **.astype()** function that is native to Pandas DataFrame columns. Also, extract the **date** portion using the **.dt.date** attribute of the **datetime64[ns]** datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76383dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dim_date = \"SELECT date_key, full_date FROM northwind_dw2.dim_date;\"\n",
    "df_dim_date = get_dataframe(user_id, pwd, host_name, src_dbname, sql_dim_date)\n",
    "df_dim_date.full_date = df_dim_date.full_date.astype('datetime64[ns]').dt.date\n",
    "df_dim_date.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d8d954",
   "metadata": {},
   "source": [
    "##### 2.5.2. Next, lookup the Surrogate Primary Key values using the corresponding Business Key,\n",
    "Next, for each **date** typed column in the Fact table, lookup the corresponding Primary Key column. Be certain to cast each **date** column to the **datetime64[ns]** data type using the **.astype()** function that's native to Pandas DataFrame columns. Also, extract the **date** portion using the **.dt.date** attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8665b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup the Surrogate Primary Key (date_key) that Corresponds to the \"order_date\" Column.\n",
    "df_dim_order_date = df_dim_date.rename(columns={\"date_key\" : \"order_date_key\", \"full_date\" : \"order_date\"})\n",
    "df_fact_orders.order_date = df_fact_orders.order_date.astype('datetime64[ns]').dt.date\n",
    "\n",
    "df_fact_orders = pd.merge(df_fact_orders, df_dim_order_date, on='order_date', how='left')\n",
    "df_fact_orders.drop(['order_date'], axis=1, inplace=True)\n",
    "df_fact_orders.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1059e88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup the Surrogate Primary Key (date_key) that Corresponds to the \"paid_date\" Column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad568c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup the Surrogate Primary Key (date_key) that Corresponds to the \"shipped_date\" Column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5392b4af-c1fc-4643-973a-39210c6fab88",
   "metadata": {},
   "source": [
    "##### 2.5.3. First, fetch the Surrogate Primary Key and the Business Key from each of the remaining Dimension tables using the **get_dataframe()** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699b01a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 'customer_key' and 'customer_id' from northwind_dw2.dim_customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ac5796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Employees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76acdfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c8c313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shippers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca4c892",
   "metadata": {},
   "source": [
    "##### 2.5.4. Next, using the Business Keys, lookup the corresponding Surrogate Primary Key values in the Dimension tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81501aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Modify 'df_fact_orders' by merging it with 'df_dim_customers' on the 'customer_id' column\n",
    "# 2. Drop the 'customer_id' column\n",
    "# 3. Display the first 2 rows of the dataframe to validate your work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfc5a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for the Employees dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4f27bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for the Product dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475faf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for the Shipper dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5d844e",
   "metadata": {},
   "source": [
    "#### 2.6. Perform any Additional Transformations\n",
    "In this step we can prepare the **df_fact_orders** DataFrame so that it defines exactly what we want to see created in the database.  Issues may include dropping unwanted columns, reordering the columns, and in our case, creating a new column to serve as the primary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b7fad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Drop the columns of no particular interest\n",
    "# 2. Reorder the remaining columns\n",
    "# 3. Insert a new column, with an ever-incrementing numeric value, to serve as the primary key.\n",
    "# 4. Display the first 2 rows of the dataframe to validate your work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd75dbc",
   "metadata": {},
   "source": [
    "#### 2.7. Write the DataFrame Back to the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67a4724",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"fact_orders\"\n",
    "primary_key = \"fact_order_key\"\n",
    "db_operation = \"insert\"\n",
    "\n",
    "set_dataframe(user_id, pwd, host_name, dst_dbname, df_fact_orders, table_name, primary_key, db_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426bac81",
   "metadata": {},
   "source": [
    "### 3.0. Demonstrate that the New Data Warehouse Exists and Contains the Correct Data\n",
    "To demonstrate the viability of your solution, author a SQL SELECT statement that returns:\n",
    "- Each Customers Last Name\n",
    "- The total amount of the order quantity associated with each customer\n",
    "- The total amount of the order unit price associated with each customer\n",
    "\n",
    "**NOTE:** *Remember that a string typed variable whose value is contained by triple-quotes (\"\"\" ... \"\"\") can preserve multi-line formatting, and that a string variable has an intrinsic **.format()** function that accepts ordered parameters that will replace tokens (e.g., {0}) in the formatted string.*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdce67e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_test = \"\"\"\n",
    "\n",
    "\"\"\".format(dst_dbname)\n",
    "\n",
    "df_test = get_dataframe(user_id, pwd, host_name, src_dbname, sql_test)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f1751e",
   "metadata": {},
   "source": [
    "### 3.1 Extra Credit: Author a Query that Returns the Total Shipping Fee per Shipper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028d6f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_test2 = \"\"\"\n",
    "\n",
    "\"\"\".format(dst_dbname)\n",
    "\n",
    "df_test2 = get_dataframe(user_id, pwd, host_name, src_dbname, sql_test2)\n",
    "df_test2.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
